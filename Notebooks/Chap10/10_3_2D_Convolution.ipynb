{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukaIvanic/deeplearning_notebook_solutions/blob/main/Notebooks/Chap10/10_3_2D_Convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook 10.3: 2D Convolution**\n",
        "\n",
        "This notebook investigates the 2D convolution operation.  It\n",
        "\n",
        "\n",
        "directly.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
      ],
      "metadata": {
        "id": "VB_crnDGASX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "# Set to print in reasonable form\n",
        "np.set_printoptions(precision=3, floatmode=\"fixed\")\n",
        "torch.set_printoptions(precision=3)"
      ],
      "metadata": {
        "id": "YAoWDUb_DezG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This routine performs convolution in PyTorch"
      ],
      "metadata": {
        "id": "eAwYWXzAElHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform convolution in PyTorch\n",
        "def conv_pytorch(image, conv_weights, stride=1, pad =1):\n",
        "  # Convert image and kernel to tensors\n",
        "  image_tensor = torch.from_numpy(image) # (batchSize, channelsIn, imageHeightIn, imageWidthIn)\n",
        "  conv_weights_tensor = torch.from_numpy(conv_weights) # (channelsOut, channelsIn, kernelHeight, kernelWidth)\n",
        "\n",
        "  # Do the convolution\n",
        "  output_tensor = torch.nn.functional.conv2d(image_tensor, conv_weights_tensor, stride=stride, padding=pad)\n",
        "\n",
        "\n",
        "  # Convert back from PyTorch and return\n",
        "  return(output_tensor.numpy()) # (batchSize channelsOut imageHeightOut imageHeightIn)"
      ],
      "metadata": {
        "id": "xsmUIN-3BlWr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll start with the simplest 2D convolution.  Just one channel in and one channel out.  A single image in the batch."
      ],
      "metadata": {
        "id": "A3Sm8bUWtDNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "# Perform convolution in numpy\n",
        "def conv_numpy_1(image, weights, pad=1):\n",
        "\n",
        "    # Perform zero padding\n",
        "    if pad != 0:\n",
        "        image = np.pad(image, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant')\n",
        "\n",
        "    # Get sizes of image array and kernel weights\n",
        "    batchSize,  channelsIn, imageHeightIn, imageWidthIn = image.shape\n",
        "    channelsOut, channelsIn, kernelHeight, kernelWidth = weights.shape\n",
        "\n",
        "    # Get size of output arrays\n",
        "    imageHeightOut = np.floor(1 + imageHeightIn - kernelHeight).astype(int)\n",
        "    imageWidthOut = np.floor(1 + imageWidthIn - kernelWidth).astype(int)\n",
        "\n",
        "    # Create output\n",
        "    out = np.zeros((batchSize, channelsOut, imageHeightOut, imageWidthOut), dtype=np.float32)\n",
        "\n",
        "    # !!!!!! NOTE THERE IS A SUBTLETY HERE !!!!!!!!\n",
        "    # I have padded the image with zeros above, so it is surrouned by a \"ring\" of zeros\n",
        "    # That means that the image indexes are all off by one\n",
        "    # This actually makes your code simpler\n",
        "\n",
        "    for c_y in range(imageHeightOut):\n",
        "      for c_x in range(imageWidthOut):\n",
        "            # TODO -- Retrieve the image pixel and the weight from the convolution\n",
        "            # Only one image in batch, one input channel and one output channel, so these indices should all be zero\n",
        "            # Replace the two lines below\n",
        "        this_pixel_value = image[0, 0, c_y:c_y+kernelHeight, c_x:c_x+kernelWidth]\n",
        "        this_weight = weights[0,0,:,:]\n",
        "\n",
        "\n",
        "            # Multiply these together and add to the output at this position\n",
        "        out[0, 0, c_y, c_x] = np.tensordot(this_pixel_value, this_weight)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "EF8FWONVLo1Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed so we always get same answer\n",
        "np.random.seed(1)\n",
        "n_batch = 1\n",
        "image_height = 4\n",
        "image_width = 6\n",
        "channels_in = 1\n",
        "kernel_size = 3\n",
        "channels_out = 1\n",
        "\n",
        "# Create random input image\n",
        "input_image= np.random.normal(size=(n_batch, channels_in, image_height, image_width))\n",
        "# Create random convolution kernel weights\n",
        "conv_weights = np.random.normal(size=(channels_out, channels_in, kernel_size, kernel_size))\n",
        "\n",
        "# Perform convolution using PyTorch\n",
        "conv_results_pytorch = conv_pytorch(input_image, conv_weights, stride=1, pad=1)\n",
        "print(\"PyTorch Results\")\n",
        "print(conv_results_pytorch)\n",
        "\n",
        "# Perform convolution in numpy\n",
        "print(\"Your results\")\n",
        "conv_results_numpy = conv_numpy_1(input_image, conv_weights)\n",
        "print(conv_results_numpy)"
      ],
      "metadata": {
        "id": "iw9KqXZTHN8v",
        "outputId": "f9d34e31-594b-4b96-ad30-5d02932ebbc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_tensor: tensor([[[[ 1.624, -0.612, -0.528, -1.073,  0.865, -2.302],\n",
            "          [ 1.745, -0.761,  0.319, -0.249,  1.462, -2.060],\n",
            "          [-0.322, -0.384,  1.134, -1.100, -0.172, -0.878],\n",
            "          [ 0.042,  0.583, -1.101,  1.145,  0.902,  0.502]]]],\n",
            "       dtype=torch.float64), conv_weights_tensor: tensor([[[[ 0.901, -0.684, -0.123],\n",
            "          [-0.936, -0.268,  0.530],\n",
            "          [-0.692, -0.397, -0.687]]]], dtype=torch.float64)\n",
            "output_tensor: tensor([[[[-0.929, -2.760,  0.716,  0.114,  0.560, -0.387],\n",
            "          [-1.515,  0.283,  1.008,  0.466, -1.094,  2.004],\n",
            "          [-1.634,  3.555, -2.154, -0.892, -1.856,  2.299],\n",
            "          [ 0.565, -0.947, -0.629,  2.996, -1.811, -0.533]]]],\n",
            "       dtype=torch.float64)\n",
            "PyTorch Results\n",
            "[[[[-0.929 -2.760  0.716  0.114  0.560 -0.387]\n",
            "   [-1.515  0.283  1.008  0.466 -1.094  2.004]\n",
            "   [-1.634  3.555 -2.154 -0.892 -1.856  2.299]\n",
            "   [ 0.565 -0.947 -0.629  2.996 -1.811 -0.533]]]]\n",
            "Your results\n",
            "[[[[-0.929 -2.760  0.716  0.114  0.560 -0.387]\n",
            "   [-1.515  0.283  1.008  0.466 -1.094  2.004]\n",
            "   [-1.634  3.555 -2.154 -0.892 -1.856  2.299]\n",
            "   [ 0.565 -0.947 -0.629  2.996 -1.811 -0.533]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now add in the possibility of using different strides"
      ],
      "metadata": {
        "id": "IYj_lxeGzaHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform convolution in numpy\n",
        "def conv_numpy_2(image, weights, stride=1, pad=1):\n",
        "\n",
        "    # Perform zero padding\n",
        "    if pad != 0:\n",
        "        image = np.pad(image, ((0, 0), (0 ,0), (pad, pad), (pad, pad)),'constant')\n",
        "\n",
        "    # Get sizes of image array and kernel weights\n",
        "    batchSize,  channelsIn, imageHeightIn, imageWidthIn = image.shape\n",
        "    channelsOut, channelsIn, kernelHeight, kernelWidth = weights.shape\n",
        "\n",
        "    # Get size of output arrays\n",
        "    imageHeightOut = np.floor(1 + (imageHeightIn - kernelHeight) / stride).astype(int)\n",
        "    imageWidthOut = np.floor(1 + (imageWidthIn - kernelWidth) / stride).astype(int)\n",
        "\n",
        "    # Create output\n",
        "    out = np.zeros((batchSize, channelsOut, imageHeightOut, imageWidthOut), dtype=np.float32)\n",
        "\n",
        "    for c_y in range(imageHeightOut):\n",
        "      for c_x in range(imageWidthOut):\n",
        "            # TODO -- Retrieve the image pixel and the weight from the convolution\n",
        "            # Only one image in batch, one input channel and one output channel, so these indices should all be zero\n",
        "            # Replace the two lines below\n",
        "\n",
        "\n",
        "        this_pixel_value = image[0, 0, c_y*stride:c_y*stride+kernelHeight, c_x*stride:c_x*stride+kernelWidth]\n",
        "        this_weight = weights[0,0,:,:]\n",
        "\n",
        "\n",
        "            # Multiply these together and add to the output at this position\n",
        "        out[0, 0, c_y, c_x] = np.tensordot(this_pixel_value, this_weight)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "GiujmLhqHN1F"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed so we always get same answer\n",
        "np.random.seed(1)\n",
        "n_batch = 1\n",
        "image_height = 12\n",
        "image_width = 10\n",
        "channels_in = 1\n",
        "kernel_size = 3\n",
        "channels_out = 1\n",
        "stride = 2\n",
        "\n",
        "# Create random input image\n",
        "input_image= np.random.normal(size=(n_batch, channels_in, image_height, image_width))\n",
        "# Create random convolution kernel weights\n",
        "conv_weights = np.random.normal(size=(channels_out, channels_in, kernel_size, kernel_size))\n",
        "\n",
        "# Perform convolution using PyTorch\n",
        "conv_results_pytorch = conv_pytorch(input_image, conv_weights, stride, pad=1)\n",
        "print(\"PyTorch Results\")\n",
        "print(conv_results_pytorch)\n",
        "\n",
        "# Perform convolution in numpy\n",
        "print(\"Your results\")\n",
        "conv_results_numpy = conv_numpy_2(input_image, conv_weights, stride, pad=1)\n",
        "print(conv_results_numpy)"
      ],
      "metadata": {
        "id": "FeJy6Bvozgxq",
        "outputId": "6a4a1538-0d01-491c-fc50-fcec0f86dc04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_tensor: tensor([[[[ 1.624, -0.612, -0.528, -1.073,  0.865, -2.302,  1.745, -0.761,\n",
            "            0.319, -0.249],\n",
            "          [ 1.462, -2.060, -0.322, -0.384,  1.134, -1.100, -0.172, -0.878,\n",
            "            0.042,  0.583],\n",
            "          [-1.101,  1.145,  0.902,  0.502,  0.901, -0.684, -0.123, -0.936,\n",
            "           -0.268,  0.530],\n",
            "          [-0.692, -0.397, -0.687, -0.845, -0.671, -0.013, -1.117,  0.234,\n",
            "            1.660,  0.742],\n",
            "          [-0.192, -0.888, -0.747,  1.692,  0.051, -0.637,  0.191,  2.100,\n",
            "            0.120,  0.617],\n",
            "          [ 0.300, -0.352, -1.143, -0.349, -0.209,  0.587,  0.839,  0.931,\n",
            "            0.286,  0.885],\n",
            "          [-0.754,  1.253,  0.513, -0.298,  0.489, -0.076,  1.132,  1.520,\n",
            "            2.186, -1.396],\n",
            "          [-1.444, -0.504,  0.160,  0.876,  0.316, -2.022, -0.306,  0.828,\n",
            "            0.230,  0.762],\n",
            "          [-0.222, -0.201,  0.187,  0.410,  0.198,  0.119, -0.671,  0.378,\n",
            "            0.122,  1.129],\n",
            "          [ 1.199,  0.185, -0.375, -0.639,  0.423,  0.077, -0.344,  0.044,\n",
            "           -0.620,  0.698],\n",
            "          [-0.447,  1.225,  0.403,  0.594, -1.095,  0.169,  0.741, -0.954,\n",
            "           -0.266,  0.033],\n",
            "          [-1.373,  0.315,  0.846, -0.860,  0.351, -1.312, -0.039, -1.616,\n",
            "            1.121,  0.409]]]], dtype=torch.float64), conv_weights_tensor: tensor([[[[-0.025, -0.775,  1.274],\n",
            "          [ 1.967, -1.858,  1.236],\n",
            "          [ 1.628,  0.338, -1.199]]]], dtype=torch.float64)\n",
            "output_tensor: tensor([[[[-0.809, -4.550, -5.486, -9.506, -4.512],\n",
            "          [-0.055,  1.145, -5.388, -3.910,  0.097],\n",
            "          [-0.186,  0.660,  1.630,  2.275,  4.874],\n",
            "          [ 2.386, -0.225,  3.288, -4.239, -1.403],\n",
            "          [ 0.825,  1.710, -3.246,  3.246,  1.709],\n",
            "          [ 0.809,  3.695,  3.491, -2.113, -2.714]]]], dtype=torch.float64)\n",
            "PyTorch Results\n",
            "[[[[-0.809 -4.550 -5.486 -9.506 -4.512]\n",
            "   [-0.055  1.145 -5.388 -3.910  0.097]\n",
            "   [-0.186  0.660  1.630  2.275  4.874]\n",
            "   [ 2.386 -0.225  3.288 -4.239 -1.403]\n",
            "   [ 0.825  1.710 -3.246  3.246  1.709]\n",
            "   [ 0.809  3.695  3.491 -2.113 -2.714]]]]\n",
            "Your results\n",
            "[[[[-0.809 -4.550 -5.486 -9.506 -4.512]\n",
            "   [-0.055  1.145 -5.388 -3.910  0.097]\n",
            "   [-0.186  0.660  1.630  2.275  4.874]\n",
            "   [ 2.386 -0.225  3.288 -4.239 -1.403]\n",
            "   [ 0.825  1.710 -3.246  3.246  1.709]\n",
            "   [ 0.809  3.695  3.491 -2.113 -2.714]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll introduce multiple input and output channels"
      ],
      "metadata": {
        "id": "3flq1Wan2gX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform convolution in numpy\n",
        "def conv_numpy_3(image, weights, stride=1, pad=1):\n",
        "\n",
        "    # Perform zero padding\n",
        "    if pad != 0:\n",
        "        image = np.pad(image, ((0, 0), (0 ,0), (pad, pad), (pad, pad)),'constant')\n",
        "\n",
        "    # Get sizes of image array and kernel weights\n",
        "    batchSize,  channelsIn, imageHeightIn, imageWidthIn = image.shape\n",
        "    channelsOut, channelsIn, kernelHeight, kernelWidth = weights.shape\n",
        "\n",
        "    # Get size of output arrays\n",
        "    imageHeightOut = np.floor(1 + (imageHeightIn - kernelHeight) / stride).astype(int)\n",
        "    imageWidthOut = np.floor(1 + (imageWidthIn - kernelWidth) / stride).astype(int)\n",
        "\n",
        "    # Create output\n",
        "    out = np.zeros((batchSize, channelsOut, imageHeightOut, imageWidthOut), dtype=np.float32)\n",
        "\n",
        "    for c_y in range(imageHeightOut):\n",
        "      for c_x in range(imageWidthOut):\n",
        "        for c_channel_out in range(channelsOut):\n",
        "          for c_channel_in in range(channelsIn):\n",
        "            # TODO -- Retrieve the image pixel and the weight from the convolution\n",
        "                # Only one image in batch, one input channel and one output channel, so these indices should all be zero\n",
        "                # Replace the two lines below\n",
        "\n",
        "            this_pixel_value = image[0, c_channel_in, c_y*stride:c_y*stride+kernelHeight, c_x*stride:c_x*stride+kernelWidth]\n",
        "            this_weight = weights[c_channel_out,c_channel_in,:,:]\n",
        "\n",
        "\n",
        "                # Multiply these together and add to the output at this position\n",
        "            out[0, c_channel_out, c_y, c_x] += np.tensordot(this_pixel_value, this_weight)\n",
        "\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "AvdRWGiU2ppX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed so we always get same answer\n",
        "np.random.seed(1)\n",
        "n_batch = 1\n",
        "image_height = 4\n",
        "image_width = 6\n",
        "channels_in = 5\n",
        "kernel_size = 3\n",
        "channels_out = 2\n",
        "\n",
        "# Create random input image\n",
        "input_image= np.random.normal(size=(n_batch, channels_in, image_height, image_width))\n",
        "# Create random convolution kernel weights\n",
        "conv_weights = np.random.normal(size=(channels_out, channels_in, kernel_size, kernel_size))\n",
        "\n",
        "# Perform convolution using PyTorch\n",
        "conv_results_pytorch = conv_pytorch(input_image, conv_weights, stride=1, pad=1)\n",
        "print(\"PyTorch Results\")\n",
        "print(conv_results_pytorch)\n",
        "\n",
        "# Perform convolution in numpy\n",
        "print(\"Your results\")\n",
        "conv_results_numpy = conv_numpy_3(input_image, conv_weights, stride=1, pad=1)\n",
        "print(conv_results_numpy)"
      ],
      "metadata": {
        "id": "mdSmjfvY4li2",
        "outputId": "d22e894a-ed49-4a59-8832-4b28c5744aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Results\n",
            "[[[[ -0.785   5.463  -2.480   5.026  -3.594   7.785]\n",
            "   [ -6.744   2.534  -0.664   7.149  -9.839   7.849]\n",
            "   [ -4.794  14.074  -1.060   2.706 -10.182   2.004]\n",
            "   [  1.809   0.287   4.648  -1.840   3.259   1.073]]\n",
            "\n",
            "  [[  4.150   5.372   1.699   0.500   0.589   4.361]\n",
            "   [ -4.123   5.136   4.677  -3.895  -4.990   2.546]\n",
            "   [  3.991   5.768  -2.315   8.473   1.752   2.766]\n",
            "   [  1.529   0.318  11.518  -5.444  -2.293   1.270]]]]\n",
            "Your results\n",
            "[[[[ -0.785   5.463  -2.480   5.026  -3.594   7.785]\n",
            "   [ -6.744   2.534  -0.664   7.149  -9.839   7.849]\n",
            "   [ -4.794  14.074  -1.060   2.706 -10.182   2.004]\n",
            "   [  1.809   0.287   4.648  -1.840   3.259   1.073]]\n",
            "\n",
            "  [[  4.150   5.372   1.699   0.500   0.589   4.361]\n",
            "   [ -4.123   5.136   4.677  -3.895  -4.990   2.546]\n",
            "   [  3.991   5.768  -2.315   8.473   1.752   2.766]\n",
            "   [  1.529   0.318  11.518  -5.444  -2.293   1.270]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll do the full convolution with multiple images (batch size > 1), and multiple input channels, multiple output channels."
      ],
      "metadata": {
        "id": "Q2MUFebdsJbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform convolution in numpy\n",
        "def conv_numpy_4(image, weights, stride=1, pad=1):\n",
        "\n",
        "    # Perform zero padding\n",
        "    if pad != 0:\n",
        "        image = np.pad(image, ((0, 0), (0 ,0), (pad, pad), (pad, pad)),'constant')\n",
        "\n",
        "    # Get sizes of image array and kernel weights\n",
        "    batchSize,  channelsIn, imageHeightIn, imageWidthIn = image.shape\n",
        "    channelsOut, channelsIn, kernelHeight, kernelWidth = weights.shape\n",
        "\n",
        "    # Get size of output arrays\n",
        "    imageHeightOut = np.floor(1 + (imageHeightIn - kernelHeight) / stride).astype(int)\n",
        "    imageWidthOut = np.floor(1 + (imageWidthIn - kernelWidth) / stride).astype(int)\n",
        "\n",
        "    # Create output\n",
        "    out = np.zeros((batchSize, channelsOut, imageHeightOut, imageWidthOut), dtype=np.float32)\n",
        "\n",
        "    for c_batch in range(batchSize):\n",
        "      for c_y in range(imageHeightOut):\n",
        "        for c_x in range(imageWidthOut):\n",
        "          for c_channel_out in range(channelsOut):\n",
        "            for c_channel_in in range(channelsIn):\n",
        "              # TODO -- Retrieve the image pixel and the weight from the convolution\n",
        "                  # Only one image in batch, one input channel and one output channel, so these indices should all be zero\n",
        "                  # Replace the two lines below\n",
        "\n",
        "              this_pixel_value = image[c_batch, c_channel_in, c_y*stride:c_y*stride+kernelHeight, c_x*stride:c_x*stride+kernelWidth]\n",
        "              this_weight = weights[c_channel_out,c_channel_in,:,:]\n",
        "\n",
        "\n",
        "                  # Multiply these together and add to the output at this position\n",
        "              out[c_batch, c_channel_out, c_y, c_x] += np.tensordot(this_pixel_value, this_weight)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "5WePF-Y-sC1y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1w2GEBtqAM2P",
        "outputId": "3fc571ab-3dd5-44fb-f432-8d456805b5d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Results\n",
            "[[[[ -3.633  -1.644   0.169  -1.167  -3.865   6.045]\n",
            "   [ -9.004   7.303   4.414   0.361  -6.739   3.939]\n",
            "   [ -1.391  13.502   3.807  -9.379   3.991   5.442]\n",
            "   [  2.805   6.874  -9.287  -4.468  -1.501   4.607]]\n",
            "\n",
            "  [[  1.940  -1.410   2.397  -0.235  -0.394  -1.483]\n",
            "   [  5.049  -3.335  -7.596  -1.586   3.049  -1.857]\n",
            "   [  3.514   0.475  -1.952  -1.291  -0.589  -0.948]\n",
            "   [  6.524  -0.020  -3.298  -1.248   3.249  -2.680]]]\n",
            "\n",
            "\n",
            " [[[  4.154  -4.764  11.635   0.506  -4.012  -2.081]\n",
            "   [ -1.125  -0.677  16.749  -7.030  -5.978  -2.629]\n",
            "   [  0.778  -3.984 -10.284   1.575  -8.888   1.163]\n",
            "   [  0.556  -2.290   1.407  -3.088   2.227  -5.403]]\n",
            "\n",
            "  [[  1.048   4.322  -0.901  -5.820   3.998   2.281]\n",
            "   [ -1.313   8.409  -0.100  14.625   1.223  -3.572]\n",
            "   [  1.411   1.617  -4.078  -8.107   3.705   0.229]\n",
            "   [ -3.540  -5.292  -5.619  -4.039  -4.048  -3.446]]]]\n",
            "Your results\n",
            "[[[[ -3.633  -1.644   0.169  -1.167  -3.865   6.045]\n",
            "   [ -9.004   7.303   4.414   0.361  -6.739   3.939]\n",
            "   [ -1.391  13.502   3.807  -9.379   3.991   5.442]\n",
            "   [  2.805   6.874  -9.287  -4.468  -1.501   4.607]]\n",
            "\n",
            "  [[  1.940  -1.410   2.397  -0.235  -0.394  -1.483]\n",
            "   [  5.049  -3.335  -7.596  -1.586   3.049  -1.857]\n",
            "   [  3.514   0.475  -1.952  -1.291  -0.589  -0.948]\n",
            "   [  6.524  -0.020  -3.298  -1.248   3.249  -2.680]]]\n",
            "\n",
            "\n",
            " [[[  4.154  -4.764  11.635   0.506  -4.012  -2.081]\n",
            "   [ -1.125  -0.677  16.749  -7.030  -5.978  -2.629]\n",
            "   [  0.778  -3.984 -10.284   1.575  -8.888   1.163]\n",
            "   [  0.556  -2.290   1.407  -3.088   2.227  -5.403]]\n",
            "\n",
            "  [[  1.048   4.322  -0.901  -5.820   3.998   2.281]\n",
            "   [ -1.313   8.409  -0.100  14.625   1.223  -3.572]\n",
            "   [  1.411   1.617  -4.078  -8.107   3.705   0.229]\n",
            "   [ -3.540  -5.292  -5.619  -4.039  -4.048  -3.446]]]]\n"
          ]
        }
      ],
      "source": [
        "# Set random seed so we always get same answer\n",
        "np.random.seed(1)\n",
        "n_batch = 2\n",
        "image_height = 4\n",
        "image_width = 6\n",
        "channels_in = 5\n",
        "kernel_size = 3\n",
        "channels_out = 2\n",
        "\n",
        "# Create random input image\n",
        "input_image= np.random.normal(size=(n_batch, channels_in, image_height, image_width))\n",
        "# Create random convolution kernel weights\n",
        "conv_weights = np.random.normal(size=(channels_out, channels_in, kernel_size, kernel_size))\n",
        "\n",
        "# Perform convolution using PyTorch\n",
        "conv_results_pytorch = conv_pytorch(input_image, conv_weights, stride=1, pad=1)\n",
        "print(\"PyTorch Results\")\n",
        "print(conv_results_pytorch)\n",
        "\n",
        "# Perform convolution in numpy\n",
        "print(\"Your results\")\n",
        "conv_results_numpy = conv_numpy_4(input_image, conv_weights, stride=1, pad=1)\n",
        "print(conv_results_numpy)"
      ]
    }
  ]
}